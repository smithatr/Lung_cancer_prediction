# -*- coding: utf-8 -*-
"""lung-cancer-detection-using-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HW5iPuKwv3s82XSm1kp87_FiTLN_Sv9d

##**Importing Libraries**
"""

# Import Libraries

from sklearn.ensemble import RandomForestClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn import svm
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

#Environment check
import os
import warnings
warnings.filterwarnings("ignore")

"""##**Let's get the data</span>**"""

import numpy as np
import pandas as pd
for dirname, _, filenames in os.walk('/content/survey lung cancer.csv'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

df = pd.read_csv('/content/survey lung cancer.csv')

df.info()

df.describe()

df.columns

df.head()

"""##**Feature engineering and mapping</span>**

> This includes setting strings like Yes/No to integers which match with datatype of other parameters.

"""

df['LUNG_CANCER']=df['LUNG_CANCER'].map({'YES':2,'NO':1})

df['GENDER']=df['GENDER'].map({'M':1,'F':2})

df.head()

"""## **Checking Duplicate dataset**"""

df[df.duplicated()]

df.drop_duplicates()

"""## **Spliting the dataset**"""

X = df.iloc[:,:-1]
y = df['LUNG_CANCER']

X.shape

y.shape

#Split the data into train and test set (approx 80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2, stratify = y)

"""## **Scaling Data**

Since, all value points do not lie in same range, lets scale the data first
"""

from sklearn.preprocessing import MinMaxScaler

scale=MinMaxScaler()
X_train_scaled=pd.DataFrame(scale.fit_transform(X_train),columns=X_train.columns)
X_train_scaled

"""**<span style="color:#FFC72C;">We can see that all data points are now in floating points.</span>**"""

X_test_scaled = pd.DataFrame(scale.fit_transform(X_test),columns=X_test.columns)
X_test_scaled

"""## **Creating our classification model**

## **<span style="color:#cd486b;"> 1️⃣ Random Forest Classifier</span>**
"""

model_1 = RandomForestClassifier()
# Fit
model_1.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error
# Get predictions
predictions_1 = model_1.predict(X_test)

# Calculate MAE
mae_1 = mean_absolute_error(predictions_1 ,y_test )

print("Mean Absolute Error with  Random Forest classifier:" , mae_1)

"""## **<span style="color:#cd486b;">2️⃣ Support Vector Machine Classifier</span>**"""

model_2 = svm.SVC()
# Fit
model_2.fit(X_train, y_train)

# Make predictions calculate mean absolute error

predictions_2 = model_2.predict(X_test)
mae_2 = mean_absolute_error(predictions_2, y_test)

print("Mean Absolute Error with Support Vector Machine: {:,.0f}".format(mae_2))

"""## **<span style="color:#cd486b;">3️⃣ K Nearest neighbors </span>**"""

model_3 = KNeighborsClassifier(n_neighbors=5)
# Fit
model_3.fit(X_train, y_train)

# Make predictions calculate mean absolute error

predictions_3 = model_3.predict(X_test)
mae_3 = mean_absolute_error(predictions_3, y_test)

print("Mean Absolute Error with K nearest Neighbor classifier is : {:,.0f}".format(mae_3))

"""**<span style="color:#FFC72C;">The lower the value the better and 0 means the model is perfect.</span>**

## **Predictions**
"""

print('Random Forest classifier Predictions - ', predictions_1)

print('Support Vector Machine classifier predictions - ', predictions_2)

print('K nearest neighbor classifier Predictions - ', predictions_3)

from sklearn.metrics import PrecisionRecallDisplay

display = PrecisionRecallDisplay.from_estimator(
    model_2, X_test, y_test, name="LinearSVC"
)
_ = display.ax_.set_title("Precision-Recall curve")

import sklearn

#precision score

precision_score_1 = sklearn.metrics.precision_score(y_test, predictions_1, labels=model_1.classes_)
precision_score_2 = sklearn.metrics.precision_score(y_test, predictions_2, labels=model_2.classes_)
precision_score_3 = sklearn.metrics.precision_score(y_test, predictions_3, labels=model_3.classes_)

print("Precision score for Random Forest Classifier is  ", precision_score_1)

print("Precision score for Support Vector Machine classifier is  ", precision_score_2)

print("Precision score for K nearest neighbor classifier is  ", precision_score_3)

F1_model_1 = sklearn.metrics.f1_score(y_test, predictions_1, labels=model_1.classes_, pos_label=1, average='weighted', sample_weight=None)

F1_model_2 = sklearn.metrics.f1_score(y_test, predictions_2, labels=model_2.classes_, pos_label=1, average='weighted', sample_weight = None)

F1_model_3 = sklearn.metrics.f1_score(y_test, predictions_3, labels=model_3.classes_, pos_label=1, average='weighted', sample_weight = None)

print("F1 Score for Random Forest Classifier is ", F1_model_1)
print("F1 Score for Support Vector Machine classifier is ", F1_model_2)
print("F1 Score for K nearest neighbor classifier is ", F1_model_3)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC

#clf = SVC(random_state=0)
#clf.fit(X_train, y_train)

cm = confusion_matrix(y_test, predictions_2, labels=model_2.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=model_2.classes_)
disp.plot()

#plt.show()

"""**Here, 2 means accurate prediction of Lung Cancer**

## **Accuracy Check**
"""

from sklearn import metrics

accuracy_1 = metrics.accuracy_score(y_test,predictions_1)
print('Accuracy for Random Forest classifier model is - ', accuracy_1)

accuracy_2 = metrics.accuracy_score(y_test,predictions_2)
print('Accuracy for Support Vector Machine classifier model is - ', accuracy_2)

accuracy_3 = metrics.accuracy_score(y_test,predictions_3)
print('Accuracy for K nearest neighbor classifier model is - ', accuracy_3)

"""**Since, the precision value for all above models is not optimum as compared to accuracy values. Thus, we can say that accuracy is not the metric that we would like to consider for this application. So, there is clearly some problem.**

# **Finding the problem**
"""

y.value_counts()

"""## **Imbalanced dataset**

Imbalanced datasets are those where there is a severe skew in the class distribution, such as 1:100 or 1:1000 examples in the minority class to the majority class i.e number of Yes(2) labels are more than number of No(1) labels.

# **Solution**

**<span style="color:#000000;">Let's try some Resampling techniques to equalise the labels.</span>**

## **<span style="color:#cd486b;">1️⃣ Under Sampling - Randomly delete examples in the majority class</span>**

**<span style="color:#cd486b;">Decresing number of samples in large class and equate them to number of samples in  the lower class with less samples.</span>**
"""

# Let's try balanced Random Forest Classifier to balance the imbalanced data

brf = BalancedRandomForestClassifier(n_estimators = 100, random_state = 0)

brf.fit(X_train, y_train)

print("F1 score with Balanced Random Forest Classifier is ",sklearn.metrics.f1_score(y_test, brf.predict(X_test)))

pre_score = sklearn.metrics.precision_score(y_test, brf.predict(X_test))
print("Precision score for Balanced Random Forest Classifier is  ", pre_score)

#Confusion Matrix

cm = confusion_matrix(y_test, brf.predict(X_test), labels=brf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=brf.classes_)
disp.plot()

"""## **<span style="color:#cd486b;">2️⃣ Over Sampling - Randomly duplicate examples in the minority class.</span>**"""

from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import RandomOverSampler

#y labels are - 270: 39 (divide both by 309 to get weights)
X, y = make_classification(n_classes = 2, class_sep = 2, weights = [0.87, 0.12],
                           n_informative = 3, n_redundant = 1, flip_y = 0, n_features = 20,
                          n_clusters_per_class = 1, n_samples = 309, random_state = 10)

print('Orignal dataset shape %s' % Counter(y))

ros = RandomOverSampler(random_state = 42)
X_res, y_res = ros.fit_resample(X, y)

print('Reshaped dataset shape %s' % Counter(y_res))

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state = 42, test_size = 0.2, stratify = y_res)

model = RandomForestClassifier()
model.fit(X_train, y_train)

predictions = model.predict(X_test)

model_precision_score = sklearn.metrics.precision_score(y_test, predictions, labels=model.classes_)
print("Precision score after using Balanced Random Forest Classifier is  ", model_precision_score)

print("F1 score after Balanced Random Forest Classifier is ",sklearn.metrics.f1_score(y_test, model.predict(X_test)))

#Confusion Matrix

cm = confusion_matrix(y_test, model.predict(X_test), labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=model.classes_)
disp.plot()

"""**<span style="color:#cd486b;">Using oversampling technique we were able to get high precision and recall values along with high F1 score of 0.981 which can be easily witnessed using the above confusion matrix.</span>**



"""